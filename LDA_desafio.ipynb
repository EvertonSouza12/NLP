{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCkerWnPF8LCaO95eadPpR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD3MWUbeDu6s"
      },
      "outputs": [],
      "source": [
        "doc1 = \"Excelente academia! Equipamentos de alta qualidade e instrutores muito prestativos.\"\n",
        "doc2 = \"Adoro treinar aqui. O ambiente é motivador e as aulas em grupo são incríveis.\"\n",
        "doc3 = \"Ótimo lugar para alcançar meus objetivos fitness. Sempre limpo e organizado.\"\n",
        "doc4 = \"A equipe da academia é super amigável e sempre disposta a ajudar com dicas de treino.\"\n",
        "doc5 = \"Treino aqui há meses e já vejo resultados impressionantes. Recomendo!\"\n",
        "doc6 = \"A variedade de equipamentos é incrível, nunca fico entediado com minha rotina.\"\n",
        "doc7 = \"As instalações são modernas e bem cuidadas. Sempre me sinto confortável treinando aqui.\"\n",
        "doc8 =\"Participar das aulas de ioga me trouxe paz e flexibilidade. Uma adição incrível à academia.\"\n",
        "doc9 = \"Os horários flexíveis facilitam encaixar os treinos na minha agenda lotada.\"\n",
        "doc10 = \"Estou muito feliz com minha adesão. Finalmente encontrei uma academia que se encaixa perfeitamente no que eu procurava.\"\n",
        "\n",
        "\n",
        "doc_set = [doc1, doc2, doc3, doc4, doc5, doc6, doc7, doc8, doc9, doc10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "tokenziner = RegexpTokenizer(r'\\w+')\n",
        "tokens = [nltk.word_tokenize(line) for line in doc_set]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rwDRXxEEphO",
        "outputId": "a43c0562-4c39-427b-b49d-1e7b45ab855c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Excelente', 'academia', '!', 'Equipamentos', 'de', 'alta', 'qualidade', 'e', 'instrutores', 'muito', 'prestativos', '.'], ['Adoro', 'treinar', 'aqui', '.', 'O', 'ambiente', 'é', 'motivador', 'e', 'as', 'aulas', 'em', 'grupo', 'são', 'incríveis', '.'], ['Ótimo', 'lugar', 'para', 'alcançar', 'meus', 'objetivos', 'fitness', '.', 'Sempre', 'limpo', 'e', 'organizado', '.'], ['A', 'equipe', 'da', 'academia', 'é', 'super', 'amigável', 'e', 'sempre', 'disposta', 'a', 'ajudar', 'com', 'dicas', 'de', 'treino', '.'], ['Treino', 'aqui', 'há', 'meses', 'e', 'já', 'vejo', 'resultados', 'impressionantes', '.', 'Recomendo', '!'], ['A', 'variedade', 'de', 'equipamentos', 'é', 'incrível', ',', 'nunca', 'fico', 'entediado', 'com', 'minha', 'rotina', '.'], ['As', 'instalações', 'são', 'modernas', 'e', 'bem', 'cuidadas', '.', 'Sempre', 'me', 'sinto', 'confortável', 'treinando', 'aqui', '.'], ['Participar', 'das', 'aulas', 'de', 'ioga', 'me', 'trouxe', 'paz', 'e', 'flexibilidade', '.', 'Uma', 'adição', 'incrível', 'à', 'academia', '.'], ['Os', 'horários', 'flexíveis', 'facilitam', 'encaixar', 'os', 'treinos', 'na', 'minha', 'agenda', 'lotada', '.'], ['Estou', 'muito', 'feliz', 'com', 'minha', 'adesão', '.', 'Finalmente', 'encontrei', 'uma', 'academia', 'que', 'se', 'encaixa', 'perfeitamente', 'no', 'que', 'eu', 'procurava', '.']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = stopwords.words('portuguese')\n",
        "\n",
        "#stopped_tokens = [i for i in tokens if not i in stopwords]\n",
        "for i in tokens:\n",
        "  if i not in stopwords:\n",
        "    tokens.append(i)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRhBQbHJG01w",
        "outputId": "c6a00009-76b7-45c6-bc33-97d8f391d0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "p_stemmer = PorterStemmer()\n",
        "\n",
        "texto = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "print(stopped_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "wlLvJ-HNI3jV",
        "outputId": "13d9ee9c-461e-48ce-9c05-c29b6eb579d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3c5e93f904c7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mp_stemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtexto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopped_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopped_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-3c5e93f904c7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mp_stemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtexto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopped_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopped_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word, to_lowercase)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mto_lowercase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mto_lowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mword\u001b[0m \u001b[0malways\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \"\"\"\n\u001b[0;32m--> 658\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_lowercase\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLTK_EXTENSIONS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoplist = set('for a of the and to in'.split(' '))\n",
        "texts = [[word for word in document.lower().split() if\n",
        "word not in stoplist]\n",
        " for document in doc_set]\n",
        "from collections import defaultdict\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        " for token in text:\n",
        "  frequency[token] += 1\n",
        "\n",
        "\n",
        "processed_corpus = [[token for token in text if\n",
        "frequency[token] > 1] for text in texts]\n",
        "print(processed_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sqxmKGAKMQu",
        "outputId": "e13a3cbb-35c8-4479-db51-3c664d3eb906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['equipamentos', 'de', 'e', 'muito'], ['aqui.', 'é', 'e', 'as', 'aulas', 'são'], ['sempre', 'e'], ['academia', 'é', 'e', 'sempre', 'com', 'de'], ['e'], ['de', 'equipamentos', 'é', 'com', 'minha'], ['as', 'são', 'e', 'sempre', 'me', 'aqui.'], ['aulas', 'de', 'me', 'e', 'uma'], ['os', 'os', 'minha'], ['muito', 'com', 'minha', 'uma', 'academia', 'que', 'que']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(processed_corpus)\n",
        "print(dictionary)\n",
        "print(dictionary.token2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGejScUhKeWF",
        "outputId": "d4e0636e-7874-44cf-d77f-750b329d9828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary<17 unique tokens: ['de', 'e', 'equipamentos', 'muito', 'aqui.']...>\n",
            "{'de': 0, 'e': 1, 'equipamentos': 2, 'muito': 3, 'aqui.': 4, 'as': 5, 'aulas': 6, 'são': 7, 'é': 8, 'sempre': 9, 'academia': 10, 'com': 11, 'minha': 12, 'me': 13, 'uma': 14, 'os': 15, 'que': 16}\n"
          ]
        }
      ]
    }
  ]
}